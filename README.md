# ğŸ› ï¸ Odoo ETL & Analytics Pipeline

This project builds a complete data engineering pipeline using Odoo 15 as the data source. The pipeline extracts data via XML-RPC, transforms it using Python, loads it into PostgreSQL, and visualizes it through Apache Superset. All components are containerized using Docker and orchestrated via Apache Airflow.

---

## ğŸ“¦ Features

- Odoo ERP (Sales, Inventory, Customers modules)
- XML-RPC data extraction
- Python ETL scripts
- PostgreSQL data warehouse
- Apache Airflow for orchestration
- Apache Superset dashboards
- Docker Compose deployment

---

## ğŸª° Tech Stack

- Odoo 15 (Dockerized)
- Python 3
- PostgreSQL 13
- Apache Airflow 2.7
- Apache Superset 2.1
- Docker Compose

---

## ğŸš€ Quick Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/odooops-insight.git
cd odooops-insight
```

### 2. Set Up Environment Variables

Create a `.env` file in the root directory:

```env
ODOO_URL=http://localhost:8069
ODOO_DB=odooops_db
ODOO_USERNAME=odoo
ODOO_PASSWORD=odoo

PG_HOST=analytics-db
PG_PORT=5432
PG_DB=analytics
PG_USER=analyst
PG_PASSWORD=analyst
```

### 3. Build and Start the Containers

```bash
docker-compose up --build
```

This will:

- Start Odoo
- Initialize PostgreSQL databases
- Launch Airflow (scheduler + webserver)
- Launch Superset

### 4. Odoo Setup (Post-Docker Boot)

After running `docker-compose up --build`, follow these steps to initialize and populate Odoo with sample data:

## ğŸ–¥ï¸ Open your browser and go to:
[http://localhost:8069](http://localhost:8069)

## ğŸ§¾ On the first load, youâ€™ll see the Odoo database creation screen:

- **Database Name**: `odooops_db`
- **Email**: `odoo@example.com`
- **Password**: `odoo`
- **Confirm Password**: `odoo`
- **Language**: English
- **Country**: Your choice

**Uncheck** `"Load demonstration data"` if you're adding custom data.  
Click **Create database**.

---

## ğŸšª Once logged in, install the required Odoo modules:

- **Sales**
- **Inventory**
- **Contacts**

Navigate to **Apps**, remove the â€œAppsâ€ filter (if nothing shows up), then click **Install** on the modules listed.

---

## ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Create or import test data:

- Add a few **customers**
- Create a few **products**
- Make some **Sales Orders** using those products/customers

This manual setup populates the models used by the ETL pipeline:

- `res.partner`
- `product.product`
- `sale.order`
- `sale.order.line`

---

Once done, you're ready to trigger the ETL pipeline.


### 5. Set Up Airflow & Superset Automatically

Airflow admin credentials:

```pgsql
username: admin
password: admin
```

Superset admin credentials:

```pgsql
username: admin
password: admin
```

### 6. Trigger the ETL Pipeline (Optional)

If you're not using Airflow UI yet, run:

```bash
docker exec -it <airflow-webserver-container-name> bash
python3 -m etl.run_extracts
python3 -m etl.load_to_postgres
```

Otherwise, trigger it via the Airflow web UI: [http://localhost:8080](http://localhost:8080)

---

## ğŸ¥ª Directory Structure

```bash
odooops-insight/
â”œâ”€â”€ airflow_dags/           # DAGs for Airflow
â”‚   â””â”€â”€ odoo_etl_dags.py    # Main ETL DAG
â”œâ”€â”€ config/                 # Configuration settings
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py           # Environment variable loader
â”œâ”€â”€ etl/                    # ETL scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ connector.py        # Odoo XML-RPC connector
â”‚   â”œâ”€â”€ extractor.py        # Data extractor logic
â”‚   â”œâ”€â”€ transform.py        # Data transformations
â”‚   â”œâ”€â”€ load_to_postgres.py # Load to PostgreSQL
â”‚   â””â”€â”€ run_extracts.py     # ETL runner script
â”œâ”€â”€ logs/                   # Airflow logs
â”œâ”€â”€ odoo/                   # Odoo addons (optional)
â”œâ”€â”€ outputs/                # CSV outputs from ETL
â”œâ”€â”€ tests/                  # Tests ETL
â”œâ”€â”€ .env                    # Environment variables (not committed)
â”œâ”€â”€ docker-compose.yml      # Docker Compose config
â”œâ”€â”€ README.md               # This file
â””â”€â”€ requirements.txt        # Python dependencies

---

## ğŸ“Š Superset Setup Notes

After Superset starts, go to [http://localhost:8088](http://localhost:8088) and log in,then:

1. Go to Settings > Database Connections
2. Click + Database
3. Use these credentials:

```yaml
Host: analytics-db
Port: 5432
Database: analytics
Username: analyst
Password: analyst
```
4. Click Test Connection, then Connect
5. Import tables and create charts/dashboards

---

## ğŸ‘©â€ğŸ’» Author

**Fatimah Naqib** â€“ Backend Engineer | Data Engineer

---
