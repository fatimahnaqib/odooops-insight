# üõ†Ô∏è Odoo ETL & Analytics Pipeline

This project builds a complete data engineering pipeline using Odoo 15 as the data source. The pipeline extracts data via XML-RPC, transforms it using Python, loads it into PostgreSQL, and visualizes it through Apache Superset. All components are containerized using Docker and orchestrated via Apache Airflow.

---

## üì¶ Features

- Odoo ERP (Sales, Inventory, Customers modules)
- XML-RPC data extraction
- Python ETL scripts
- PostgreSQL data warehouse
- Apache Airflow for orchestration
- Apache Superset dashboards
- Docker Compose deployment

---

## ü™∞ Tech Stack

- Odoo 15 (Dockerized)
- Python 3
- PostgreSQL 13
- Apache Airflow 2.7
- Apache Superset 2.1
- Docker Compose

---

## üöÄ Quick Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/fatimahnaqib/odooops-insight.git
cd odooops-insight
```

### 2. Set Up Environment Variables

Create a `.env` file in the root directory:

```env
ODOO_URL=http://localhost:8069
ODOO_DB=odooops_db
ODOO_USERNAME=odoo
ODOO_PASSWORD=odoo

PG_HOST=analytics-db
PG_PORT=5432
PG_DB=analytics
PG_USER=analyst
PG_PASSWORD=analyst
```

### 3. Build and Start the Containers

```bash
docker-compose up --build
```

This will:

- Start Odoo
- Initialize PostgreSQL databases
- Launch Airflow (scheduler + webserver)
- Launch Superset

### 4. Odoo Setup (Post-Docker Boot)

After running `docker-compose up --build`, follow these steps to initialize and populate Odoo with sample data:

## üñ•Ô∏è Open your browser and go to:
[http://localhost:8069](http://localhost:8069)

## üßæ On the first load, you‚Äôll see the Odoo database creation screen:

- **Database Name**: `odooops_db`
- **Email**: `odoo@example.com`
- **Password**: `odoo`
- **Confirm Password**: `odoo`
- **Language**: English
- **Country**: Your choice

**Uncheck** `"Load demonstration data"` if you're adding custom data.  
Click **Create database**.

---

## üö™ Once logged in, install the required Odoo modules:

- **Sales**
- **Inventory**
- **Contacts**

Navigate to **Apps**, remove the ‚ÄúApps‚Äù filter (if nothing shows up), then click **Install** on the modules listed.

---

## Create or import test data:

- Add a few **customers**
- Create a few **products**
- Make some **Sales Orders** using those products/customers

This manual setup populates the models used by the ETL pipeline:

- `res.partner`
- `product.product`
- `sale.order`
- `sale.order.line`

---

Once done, you're ready to trigger the ETL pipeline.


### 5. Set Up Airflow & Superset Automatically

Airflow admin credentials:

```pgsql
username: admin
password: admin
```

Superset admin credentials:

```pgsql
username: admin
password: admin
```

### 6. Trigger the ETL Pipeline (Optional)

If you're not using Airflow UI yet, run:

```bash
docker exec -it <airflow-webserver-container-name> bash
python3 -m etl.run_extracts
python3 -m etl.load_to_postgres
```

Otherwise, trigger it via the Airflow web UI: [http://localhost:8080](http://localhost:8080)

---

## ü•™ Directory Structure

```bash
odooops-insight/
‚îú‚îÄ‚îÄ airflow_dags/           # DAGs for Airflow
‚îÇ   ‚îî‚îÄ‚îÄ odoo_etl_dags.py    # Main ETL DAG
‚îú‚îÄ‚îÄ config/                 # Configuration settings
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ config.py           # Environment variable loader
‚îú‚îÄ‚îÄ etl/                    # ETL scripts
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ connector.py        # Odoo XML-RPC connector
‚îÇ   ‚îú‚îÄ‚îÄ extractor.py        # Data extractor logic
‚îÇ   ‚îú‚îÄ‚îÄ transform.py        # Data transformations
‚îÇ   ‚îú‚îÄ‚îÄ load_to_postgres.py # Load to PostgreSQL
‚îÇ   ‚îî‚îÄ‚îÄ run_extracts.py     # ETL runner script
‚îú‚îÄ‚îÄ logs/                   # Airflow logs
‚îú‚îÄ‚îÄ odoo/                   # Odoo addons (optional)
‚îú‚îÄ‚îÄ outputs/                # CSV outputs from ETL
‚îú‚îÄ‚îÄ tests/                  # Tests ETL
‚îú‚îÄ‚îÄ .env                    # Environment variables (not committed)
‚îú‚îÄ‚îÄ docker-compose.yml      # Docker Compose config
‚îú‚îÄ‚îÄ README.md               # This file
‚îî‚îÄ‚îÄ requirements.txt        # Python dependencies
```
---

## üìä Superset Setup Notes

After Superset starts, go to [http://localhost:8088](http://localhost:8088) and log in,then:

1. Go to Settings > Database Connections
2. Click + Database
3. Use these credentials:

```yaml
Host: analytics-db
Port: 5432
Database: analytics
Username: analyst
Password: analyst
```
4. Click Connect
5. Import tables and create charts/dashboards

---

## üë©‚Äçüíª Author

**Fatimah Naqib** ‚Äì Backend Engineer | Data Engineer

---
